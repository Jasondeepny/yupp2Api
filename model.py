import json
import requests
import os
from typing import Dict, List, Optional, Any
from dotenv import load_dotenv


# é…ç½®ç±»
class YuppConfig:
    """Yupp API é…ç½®ç®¡ç†"""

    def __init__(self):
        self.base_url = "https://yupp.ai"
        self.api_url = f"{self.base_url}/api/trpc/model.getModelInfoList,scribble.getScribbleByLabel?batch=1&input=%7B%220%22%3A%7B%22json%22%3Anull%2C%22meta%22%3A%7B%22values%22%3A%5B%22undefined%22%5D%7D%7D%2C%221%22%3A%7B%22json%22%3A%7B%22label%22%3A%22homepage_banner%22%7D%7D%7D"

    def get_headers(self) -> Dict[str, str]:
        """è·å–å¿…è¦çš„è¯·æ±‚å¤´"""
        return {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Referer": f"{self.base_url}/",
            "Origin": self.base_url,
            "Sec-Fetch-Dest": "empty",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Site": "same-origin",
        }

    def get_cookies(self) -> Dict[str, str]:
        """ä»ç¯å¢ƒå˜é‡è·å– session token å¹¶æ„å»º cookies"""
        # ä»ç¯å¢ƒå˜é‡è·å– YUPP_TOKENS
        env_tokens = os.getenv("YUPP_TOKENS")
        if not env_tokens:
            print("è­¦å‘Š: YUPP_TOKENS ç¯å¢ƒå˜é‡æœªè®¾ç½®")
            return {}

        try:
            # æ”¯æŒé€—å·åˆ†éš”çš„å¤šä¸ªtokenï¼Œå–ç¬¬ä¸€ä¸ª
            tokens = [token.strip() for token in env_tokens.split(",") if token.strip()]
            if not tokens:
                print("è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„ token")
                return {}

            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ token
            token = tokens[0]
            return {"__Secure-yupp.session-token": token}

        except Exception as e:
            print(f"è­¦å‘Š: è§£æ YUPP_TOKENS ç¯å¢ƒå˜é‡å¤±è´¥: {e}")
            return {}


# åˆå§‹åŒ–é…ç½®
config = YuppConfig()


def fetch_model_data() -> Optional[List[Dict[str, Any]]]:
    """è·å–æ¨¡å‹æ•°æ®"""
    try:
        # åˆ›å»º session å¹¶è®¾ç½® cookies
        session = requests.Session()
        cookies = config.get_cookies()
        headers = config.get_headers()

        # è®¾ç½® cookies
        for key, value in cookies.items():
            session.cookies.set(key, value)

        print(f"æ­£åœ¨è¯·æ±‚: {config.api_url}")
        response = session.get(config.api_url, headers=headers, timeout=30)

        print(f"å“åº”çŠ¶æ€ç : {response.status_code}")

        if response.status_code != 200:
            print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return None

        # è§£æ JSON å“åº”
        print(response.text)
        response_data = response.json()
        print("æˆåŠŸè·å–å¹¶è§£æJSONæ•°æ®")

        # æå–æ¨¡å‹åˆ—è¡¨æ•°æ®
        if isinstance(response_data, list) and len(response_data) > 0:
            return response_data[0]["result"]["data"]["json"]
        else:
            print("å“åº”æ•°æ®æ ¼å¼å¼‚å¸¸")
            return None

    except requests.exceptions.RequestException as e:
        print(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return None
    except (ValueError, json.JSONDecodeError) as e:
        print(f"JSONè§£æå¤±è´¥: {e}")
        if "response" in locals():
            print(f"å“åº”å†…å®¹: {response.text[:200]}")
        return None
    except KeyError as e:
        print(f"æ•°æ®ç»“æ„è§£æå¤±è´¥: {e}")
        return None


def load_fallback_data() -> List[Dict[str, Any]]:
    """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å¤‡ç”¨æ•°æ®"""
    try:
        with open("models.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print("æœªæ‰¾åˆ° models.json å¤‡ç”¨æ–‡ä»¶")
        return []
    except json.JSONDecodeError as e:
        print(f"å¤‡ç”¨æ–‡ä»¶ JSON è§£æå¤±è´¥: {e}")
        return []


# æ¨¡å‹æ•°æ®å°†åœ¨ main() å‡½æ•°ä¸­è·å–


def generate_model_tags(item: Dict[str, Any]) -> List[str]:
    """ç”Ÿæˆæ¨¡å‹æ ‡ç­¾"""
    tags = []
    tag_mapping = {
        "isPro": "â˜€ï¸",
        "isMax": "ğŸ”¥",
        "isNew": "ğŸ†•",
        "isLive": "ğŸ¤",
        "isAgent": "ğŸ¤–",
        "isFast": "ğŸš€",
        "isReasoning": "ğŸ§ ",
        "isImageGeneration": "ğŸ¨",
    }

    for key, emoji in tag_mapping.items():
        if item.get(key, False):
            tags.append(emoji)

    # æ£€æŸ¥æ˜¯å¦æ”¯æŒé™„ä»¶
    if (
        item.get("supportedAttachmentMimeTypes")
        and len(item["supportedAttachmentMimeTypes"]) > 0
    ):
        tags.append("ğŸ“")

    return tags


def filter_and_process_models(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """è¿‡æ»¤å’Œå¤„ç†æ¨¡å‹æ•°æ®"""
    # æ”¯æŒçš„æ¨¡å‹å®¶æ—
    supported_families = [
        "GPT",
        "Claude",
        "Gemini",
        "Qwen",
        "DeepSeek",
        "Perplexity",
        "Kimi",
    ]

    processed_models = []

    for item in data:
        # è¿‡æ»¤æ¡ä»¶ï¼šæ”¯æŒçš„å®¶æ—æˆ–ç‰¹æ®ŠåŠŸèƒ½
        if (
            item.get("family") in supported_families
            or item.get("isImageGeneration")
            or item.get("isAgent")
            or item.get("isLive")
        ):

            # ç”Ÿæˆæ ‡ç­¾
            tags = generate_model_tags(item)

            # æ„å»ºæ˜¾ç¤ºåç§°
            label = item.get("label", "")
            if tags:
                label += "\n" + " | ".join(tags)

            # æ„å»ºå¤„ç†åçš„æ¨¡å‹æ•°æ®
            processed_item = {
                "id": item.get("id"),
                "name": item.get("name"),
                "label": label,
                "shortLabel": item.get("shortLabel"),
                "publisher": item.get("publisher"),
                "family": item.get("family"),
                "isPro": item.get("isPro", False),
                "isInternal": item.get("isInternal", False),
                "isMax": item.get("isMax", False),
                "isLive": item.get("isLive", False),
                "isNew": item.get("isNew", False),
                "isImageGeneration": item.get("isImageGeneration", False),
                "isAgent": item.get("isAgent", False),
                "isReasoning": item.get("isReasoning", False),
                "isFast": item.get("isFast", False),
            }
            processed_models.append(processed_item)

    return processed_models


def save_models_to_file(
    models: List[Dict[str, Any]], filename: str = "model.json"
) -> bool:
    """ä¿å­˜æ¨¡å‹æ•°æ®åˆ°æ–‡ä»¶"""
    try:
        # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
        dir_name = os.path.dirname(filename)
        if dir_name:
            os.makedirs(dir_name, exist_ok=True)

        # è‹¥æ–‡ä»¶ä¸å­˜åœ¨åˆ™å…ˆåˆ›å»º
        if not os.path.exists(filename):
            try:
                with open(filename, "x", encoding="utf-8") as _:
                    pass
                print(f"å·²åˆ›å»ºæ–‡ä»¶: {filename}")
            except FileExistsError:
                # å¹¶å‘åœºæ™¯ä¸‹å¯èƒ½è¢«å…¶ä»–è¿›ç¨‹åˆ›å»ºï¼Œå¿½ç•¥
                pass

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(models, f, indent=4, ensure_ascii=False)
        print(f"æˆåŠŸä¿å­˜ {len(models)} ä¸ªæ¨¡å‹åˆ° {filename}")
        return True
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return False


def fetch_and_save_models(filename: str = "model.json") -> bool:
    """è·å–å¹¶ä¿å­˜æ¨¡å‹æ•°æ®åˆ°æŒ‡å®šæ–‡ä»¶"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    print("=== è‡ªåŠ¨è·å– Yupp æ¨¡å‹æ•°æ® ===")

    # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
    if not os.getenv("YUPP_TOKENS"):
        print("è­¦å‘Š: YUPP_TOKENS ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œæ— æ³•è‡ªåŠ¨è·å–æ¨¡å‹æ•°æ®")
        return False

    # è·å–æ¨¡å‹æ•°æ®
    data = fetch_model_data()
    if not data:
        print("API è¯·æ±‚å¤±è´¥ï¼Œå°è¯•åŠ è½½æœ¬åœ°å¤‡ç”¨æ•°æ®...")
        data = load_fallback_data()

    # å¤„ç†æ¨¡å‹æ•°æ®
    if data:
        print(f"å¼€å§‹å¤„ç† {len(data)} ä¸ªæ¨¡å‹æ•°æ®...")
        processed_models = filter_and_process_models(data)
        return save_models_to_file(processed_models, filename)
    else:
        print("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹æ•°æ®")
        return False


def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    print("=== Yupp æ¨¡å‹æ•°æ®è·å–å·¥å…· ===")

    # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
    if not os.getenv("YUPP_TOKENS"):
        print("è­¦å‘Š: YUPP_TOKENS ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("è¯·è®¾ç½® YUPP_TOKENS ç¯å¢ƒå˜é‡ï¼Œä¾‹å¦‚ï¼š")
        print("export YUPP_TOKENS='your_token_here'")
        return False

    # è·å–æ¨¡å‹æ•°æ®
    data = fetch_model_data()
    if not data:
        print("API è¯·æ±‚å¤±è´¥ï¼Œå°è¯•åŠ è½½æœ¬åœ°å¤‡ç”¨æ•°æ®...")
        data = load_fallback_data()

    # å¤„ç†æ¨¡å‹æ•°æ®
    if data:
        print(f"å¼€å§‹å¤„ç† {len(data)} ä¸ªæ¨¡å‹æ•°æ®...")
        processed_models = filter_and_process_models(data)
        save_models_to_file(processed_models)
    else:
        print("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹æ•°æ®")
        return False

    return True


if __name__ == "__main__":
    main()
